import os
import uuid
from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel
from llama_cpp import Llama
from docx import Document
from fastapi.responses import FileResponse
import gradio as gr

# åˆå§‹åŒ–
app = FastAPI()


# åŠ è½½ Qwen æ¨¡å‹
llm = Llama(
    model_path="models/Qwen3-0.6B-UD-Q4_K_XL.gguf",
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=0,
    verbose=False
)

# å®šä¹‰æ•°æ®æ¨¡å‹ç±»ï¼ˆç”¨äº FastAPI æ¥æ”¶ JSON è¯·æ±‚ï¼‰
class DocumentRequest(BaseModel):
    title: str
    context: str

def generate_text(prompt: str) -> str:
    """ç»Ÿä¸€çš„æ–‡æœ¬ç”Ÿæˆå‡½æ•°"""
    output = llm(
        prompt,
        max_tokens=1024,
        temperature=0.4,
        stop=["<|endoftext|>"]
    )
    return output['choices'][0]['text'].strip()

# FastAPIç«¯ç‚¹ POSTï¼Œç”¨é€”ï¼šä¸Šä¼ å¹¶åˆ†æç°æœ‰æ–‡æ¡£
@app.post("/upload-and-analyze")
async def upload_and_analyze(file: UploadFile = File(...)):
    content = await file.read()
    file_id = str(uuid.uuid4())[:8]
    input_path = f"temp/uploaded_{file_id}.docx"
    output_path = f"output/{file_id}_åˆ†ææŠ¥å‘Š.docx"
    os.makedirs("temp", exist_ok=True)
    os.makedirs("output", exist_ok=True)

    with open(input_path, "wb") as f:
        f.write(content)
    doc = Document(input_path)
    context = "\n".join([para.text for para in doc.paragraphs])
    prompt = f"""ä½ æ˜¯ä¸€åæ–‡æ¡£åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š
1. è¯†åˆ«æ–‡æ¡£ç±»å‹å’Œæ ¸å¿ƒä¸»é¢˜
2. ç”¨ 150 å­—æ€»ç»“ä¸»è¦å†…å®¹\n\n{context}\n\nåˆ†ææŠ¥å‘Šï¼š"""
    # è°ƒç”¨æ¨¡å‹ç”Ÿæˆåˆ†æ
    output = llm(
        prompt,
        max_tokens=1024,  # 0.6Bæ¨¡å‹åªèƒ½åˆ°è¿™ä¸ªåœ°æ­¥â€¦â€¦
        temperature=0.5,
        top_p=0.9,
        repeat_penalty=1.1,
        stop=["<|endoftext|>"]
    )
    analysis = output['choices'][0]['text'].strip()
    result_doc = Document()
    result_doc.add_heading(f"æ–‡æ¡£åˆ†ææŠ¥å‘Š - {file.filename}", 0)
    result_doc.add_paragraph(analysis)
    result_doc.save(output_path)

    return {
        "message": f"âœ… åˆ†æå®Œæˆï¼š{file.filename}",
        "original_file": input_path,
        "analysis_file": output_path
    }

#å‰ç«¯ï¼šGradio é¡µé¢
def gradio_analyze(file):
    file_id = str(uuid.uuid4())[:8]
    output_path = f"output/{file_id}_åˆ†ææŠ¥å‘Š.docx"
    doc = Document(file.name)
    context = "\n".join([para.text for para in doc.paragraphs])
    prompt = f"""ä½ æ˜¯ä¸€åæ–‡æ¡£åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š
1. è¯†åˆ«æ–‡æ¡£ç±»å‹å’Œæ ¸å¿ƒä¸»é¢˜
2. ç”¨ 150 å­—æ€»ç»“ä¸»è¦å†…å®¹\n\n{context}\n\nåˆ†ææŠ¥å‘Šï¼š"""
    output = llm( #é¬¼çŸ¥é“èƒ½ä¸èƒ½åˆ æ‰æ¯•ç«Ÿå’Œå‰é¢ä¸€æ ·#
        prompt,
        max_tokens=1024,
        temperature=0.5,
        top_p=0.9,
        repeat_penalty=1.1, 
        stop=["<|endoftext|>"]
    )
    analysis = output['choices'][0]['text'].strip()
    result_doc = Document()
    result_doc.add_heading(f"æ–‡æ¡£åˆ†ææŠ¥å‘Š - {os.path.basename(file.name)}", 0)
    result_doc.add_paragraph(analysis)
    result_doc.save(output_path)
    
    return output_path

# åˆ›å»º Gradio ç•Œé¢
gradio_app = gr.Interface(
    fn=gradio_analyze,
    inputs=gr.File(label="ä¸Šä¼ æ–‡æ¡£ (.docx)", file_types=[".docx"]),
    outputs=gr.File(label="ä¸‹è½½åˆ†ææŠ¥å‘Š"),
    title="ğŸ“„ Autoch-alpha æ–‡æ¡£æ™ºèƒ½åˆ†æç³»ç»Ÿ--åŸºäºQwenå¤§æ¨¡å‹",
    description="ä¸Šä¼  Word æ–‡æ¡£ï¼ŒAI å°†è‡ªåŠ¨åˆ†æå†…å®¹å¹¶ç”Ÿæˆåˆ†ææŠ¥å‘Š",
    allow_flagging="never"
)

# å°† Gradio åº”ç”¨æŒ‚è½½åˆ° FastAPI
app = gr.mount_gradio_app(app, gradio_app, path="/gradio")

# å¯åŠ¨æœåŠ¡
if __name__ == "__main__":
    import uvicorn
    import webbrowser
    webbrowser.open("http://localhost:8000/gradio") # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¥½çƒ¦ä¸æè¿˜ä¸èƒ½è‡ªå·±è¹¦å‡ºæ¥ï¼‰
    uvicorn.run(app, host="0.0.0.0", port=8000)